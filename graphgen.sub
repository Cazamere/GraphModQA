#!/bin/bash

#SBATCH -J test_file                         # Job name
#SBATCH -o llama_7b_%j.out                  # output file (%j expands to jobID) test_file_%j.out
#SBATCH -e llama_7b_%j.err                  # error log file (%j expands to jobID) test_file_%j.err
#SBATCH --mail-type=ALL                      # Request status by email 
#SBATCH --mail-user=clc348@cornell.edu       # Email address to send results to.
#SBATCH -N 1                                 # Total number of nodes requested
#SBATCH -n 1                                 # Total number of cores requested
#SBATCH --get-user-env                       # retrieve the users login environment
#SBATCH --mem=16G                           # server memory requested (per node)
#SBATCH -t 2:00:00                           # Time limit (hh:mm:ss)
#SBATCH --partition=default_partition       # Request partition, --partition=default_partition       
#SBATCH --gres=gpu:titanrtx:4              # Type/number of GPUs needed, --gres=gpu:1080ti:1, gpu:titanrtx:4(S:0-1),lee-compute-01, gpu:a100:8(S:0-1),nlplarge-compute-01,nlplarge-lillian-highpri, gpu:a100:8(S:0-1),nlplarge-compute-01,nlplarge-lillian-highpri-interactive

# Define arguments for Python file
MODEL="llama2"
SIZE="7b"

# Run Python file with arguments
python3 model_eval.py --model $MODEL --size $SIZE