#!/bin/bash

#SBATCH -J gpt-4-0125-preview                         # Job name
#SBATCH -o gpt-4-0125-preview_%j.out                  # output file (%j expands to jobID) test_file_%j.out
#SBATCH -e gpt-4-0125-preview_%j.err                  # error log file (%j expands to jobID) test_file_%j.err
#SBATCH --mail-type=ALL                      # Request status by email 
#SBATCH --mail-user=clc348@cornell.edu       # Email address to send results to.
#SBATCH -N 1                                 # Total number of nodes requested
#SBATCH -n 1                                 # Total number of cores requested
#SBATCH --get-user-env                       # retrieve the users login environment
#SBATCH --mem=16G                           # server memory requested (per node)
#SBATCH -t 99:00:00                           # Time limit (hh:mm:ss)
#SBATCH --partition=default_partition         # Request partition, --partition=default_partition, nlplarge-lillian-highpri, nlplarge       
#SBATCH --gres=gpu:a6000:1             # Type/number of GPUs needed, --gres=gpu:1080ti:1, gpu:a100:2 gpu:titanrtx:4(S:0-1),lee-compute-01, gpu:a100:8(S:0-1),nlplarge-compute-01,nlplarge-lillian-highpri, gpu:a100:8(S:0-1),nlplarge-compute-01,nlplarge-lillian-highpri-interactive
#SBATCH --account=lee

# Define arguments for Python file
MODEL="gpt-3.5-turbo-1106" # ["llama2", "llama3", "opt", "openelm", "mistral", "mixtral", "phi", "qwen2", "gpt-4o", "gpt-4o-mini", "gpt-4-0125-preview, "gpt-3.5-turbo-0125", "gpt-3.5-turbo-1106" "llama3.1", "gemma", "palm2"]
SIZE="70B"
# non chain prompt_types: "add_edge" "remove_edge" "add_node" "remove_node" "node_count" "edge_count" "node_degree" "edge_exists" "connected_nodes" "cycle"
# chain prompt_types: "chain_1_node_count" "chain_1_edge_count" "chain_1_node_degree" "chain_1_edge_exists" "chain_1_connected_nodes" "chain_1_cycle" "chain_1_print" "chain_2_node_count" "chain_2_edge_count" "chain_2_node_degree" "chain_2_edge_exists" "chain_2_connected_nodes" "chain_2_cycle" "chain_2_print" "chain_3_node_count" "chain_3_edge_count" "chain_3_node_degree" "chain_3_edge_exists" "chain_3_connected_nodes" "chain_3_cycle" "chain_3_print" "chain_4_node_count" "chain_4_edge_count" "chain_4_node_degree" "chain_4_edge_exists" "chain_4_connected_nodes" "chain_4_cycle" "chain_4_print" "chain_5_node_count" "chain_5_edge_count" "chain_5_node_degree" "chain_5_edge_exists" "chain_5_connected_nodes" "chain_5_cycle" "chain_5_print"
#MAX_LENGTH=6000 # if the model take in a 20x20 adjacency matrix with whitespace in between each entry, that input is of size ~1600

# todo: add num_graphs as an argument

# opt: 125m, 350m, 1.3b, 6.7b # nonsense?
# openelm: 270M, 450M, 1_1B, 3B # not built for these types of questions?
# llama2: 7b, 13b, 70b 
# llama3: 8B, 70B
# mistral: 7B
# mixtral: 7B, 22B
# phi: 4k, 128k # doesn't output anything?
# qwen: 72B
# gpt-3.5: 175B, 570B, 1.3T, 175B, 570B, 1.3T
# llama3.1: 8B, 70B, 405B
# gemma: 2b, 9b, 27b

# Run Python file with arguments
python3 model_eval.py --model $MODEL --size $SIZE --prompt_types "add_edge" --ablation False --ablationType "cot"